{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate accuracy, precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import data_functions\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from already created pickle file\n",
    "words, X, dataX, y, n_words, n_vocab, index2word, word2index = data_functions.read_data_from_pickle(\"data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-758882c256a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_train_set_from_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"traintest_data.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Training set:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Test set:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter2/capstone_v2/data_functions.py\u001b[0m in \u001b[0;36mread_train_set_from_pickle\u001b[0;34m(picklefile)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_train_set_from_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter2/capstone_v2/data_functions.py\u001b[0m in \u001b[0;36mload_from_pickle\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#Save serialized objects from pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = data_functions.read_train_set_from_pickle(\"traintest_data.pickle\")\n",
    "\n",
    "print ('Training set:', X_train.shape, y_train.shape)\n",
    "print ('Test set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All\n",
    "models ={'rnn' : {'model': keras.layers.SimpleRNN, \n",
    "                  'weights_file':'./hdf5/train_weights_rnn.hdf5', \n",
    "                  'log_file': './csv/results_rnn.csv'},\n",
    "         'lstm': {'model': keras.layers.LSTM, \n",
    "                  'weights_file': './hdf5/train_weights_lstm.hdf5', \n",
    "                  'log_file': './csv/results_lstm.csv'},\n",
    "         'gru' : {'model': keras.layers.GRU, \n",
    "                  'weights_file':'./hdf5/train_weights_gru.hdf5', \n",
    "                  'log_file': './csv/results_gru.csv'}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training only\n",
    "# models ={'rnn' : {'model': keras.layers.SimpleRNN, \n",
    "#                   'weights_file':'./hdf5/weights_rnn.hdf5', \n",
    "#                   'log_file': './csv/results_rnn.csv'},\n",
    "#          'lstm': {'model': keras.layers.LSTM, \n",
    "#                   'weights_file': './hdf5/weights_lstm.hdf5', \n",
    "#                   'log_file': './csv/results_lstm.csv'},\n",
    "#          'gru' : {'model': keras.layers.GRU, \n",
    "#                   'weights_file':'./hdf5/weights_gru.hdf5', \n",
    "#                   'log_file': './csv/results_gru.csv'}\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from model_design import modelRNN\n",
    "from importlib import reload \n",
    "# reload(model_design)\n",
    "\n",
    "mdl = 'rnn'\n",
    "model = modelRNN(model_type=models[mdl]['model'])\n",
    "model_rnn=model.create_model(hidden_layer=2000, input_shape=(X.shape[1], X.shape[2]), output_shape=y.shape[1])\n",
    "model.load_weights(weights_file=models[mdl]['weights_file'])\n",
    "\n",
    "mdl = 'lstm'\n",
    "model = modelRNN(model_type=models[mdl]['model'])\n",
    "model_lstm=model.create_model(hidden_layer=1000, input_shape=(X.shape[1], X.shape[2]), output_shape=y.shape[1])\n",
    "model.load_weights(weights_file=models[mdl]['weights_file'])\n",
    "\n",
    "mdl = 'gru'\n",
    "model = modelRNN(model_type=models[mdl]['model'])\n",
    "model_gru=model.create_model(hidden_layer=1000, input_shape=(X.shape[1], X.shape[2]), output_shape=y.shape[1])\n",
    "model.load_weights(weights_file=models[mdl]['weights_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to predict from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict next word(s) using model\n",
    "def generate_from_model(model, pattern, nb=50, show_input=False):\n",
    "    if show_input:\n",
    "        print (\"Seed:\")\n",
    "        print (\"\\\"\", ' '.join([index2word[i] for i in pattern]), \"\\\"\")\n",
    "        print('--------------------')\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(nb):\n",
    "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = numpy.argmax(prediction)\n",
    "        predictions.append(index)\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to convert indexes to text\n",
    "import numpy as np\n",
    "\n",
    "def get_global_index(item):\n",
    "    for i in range(len(X)):\n",
    "        if (item==X[i]).all():\n",
    "            return(i)\n",
    "            break\n",
    "\n",
    "def true_text_indexes(start, X_set, y_set, nb):\n",
    "    tindex = []\n",
    "    if nb==1:\n",
    "        tindex.append(np.argmax(y_set[start]))\n",
    "    else:\n",
    "        start = get_global_index(X_set[start])\n",
    "        for i in range(nb):\n",
    "            tindex.append(np.argmax(y[start+i]))\n",
    "    return(tindex)\n",
    "\n",
    "def convert_index2words(indexes):\n",
    "    ttext = []\n",
    "    for i in range(len(indexes)):\n",
    "        ttext.append(index2word[indexes[i]])   \n",
    "    return(ttext)\n",
    "\n",
    "def true_text(start, X_set, y_set, nb):\n",
    "    return(convert_index2words(true_text_indexes(start, X_set, y_set, nb)))\n",
    "\n",
    "def p_accuracy(list1, list2):\n",
    "    return((np.asarray(list1)==np.asarray(list2)).sum() / len(list1) )\n",
    "\n",
    "# s = 45166\n",
    "# print(true_text_indexes(s,5))\n",
    "# print(convert_index2words(true_text_indexes(s,5)))\n",
    "# print(true_text(s, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#predict a \"nb_samples\" sequence of \"n\" words from each model\n",
    "def predict_sequence(X_set, y_set, nb_samples, n):\n",
    "    t_text, p_lstm, p_gru, p_rnn = [], [], [], []\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        sys.stdout.write('*')\n",
    "        start = i_samples[i]\n",
    "        pattern = X_set[start]\n",
    "        pattern =[np.int(i[0]*float(n_vocab)) for i in pattern]  #Calculates indexes in pattern\n",
    "#         print(' '.join(convert_index2words(pattern)))\n",
    "        \n",
    "        #append correct prediction\n",
    "        if n==1:\n",
    "            t_text.append(true_text(start, X_set, y_set, n)[0]) \n",
    "        else:\n",
    "            t_text.append(true_text(start, X_set, y_set, n)) \n",
    "#         print(true_text(start, y_set, n)[0])\n",
    "\n",
    "        #append lstm prediction\n",
    "        r = generate_from_model(model_lstm, pattern.copy(), n, show_input=False)\n",
    "        if n==1:\n",
    "            p_lstm.append(convert_index2words(r)[0])\n",
    "        else:\n",
    "            p_lstm.append(convert_index2words(r))\n",
    "\n",
    "        #append gru prediction\n",
    "        r = generate_from_model(model_gru, pattern.copy(), n, show_input=False)\n",
    "        if n==1:\n",
    "            p_gru.append(convert_index2words(r)[0]) \n",
    "        else:\n",
    "            p_gru.append(convert_index2words(r)) \n",
    "\n",
    "        #append rnn prediction\n",
    "        r = generate_from_model(model_rnn, pattern.copy(), n, show_input=False) \n",
    "        if n==1:\n",
    "            p_rnn.append(convert_index2words(r)[0])  \n",
    "        else:\n",
    "            p_rnn.append(convert_index2words(r)) \n",
    "        \n",
    "    return(t_text, p_rnn, p_lstm, p_gru)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "X_set = X_train\n",
    "y_set = y_train\n",
    "\n",
    "nb_samples = 1000\n",
    "i_samples = [np.random.randint(2, len(X_set)-1) for p in range(0, nb_samples)]\n",
    "n = 1\n",
    "\n",
    "t_text, p_rnn, p_lstm, p_gru = predict_sequence(X_set, y_set, nb_samples, n)    \n",
    "\n",
    "print('\\n Accuracy with training data')\n",
    "# print(\"Correct\", t_text)\n",
    "print(\"rnn: accuracy = \", p_accuracy(t_text,p_rnn))\n",
    "print(\"lstm: accuracy = \", p_accuracy(t_text,p_lstm))\n",
    "print(\"gru: accuracy = \", p_accuracy(t_text,p_gru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "X_set = X_test\n",
    "y_set = y_test\n",
    "\n",
    "nb_samples = len(y_test)\n",
    "i_samples = [np.random.randint(2, len(X_set)-1) for p in range(0, nb_samples)]\n",
    "n = 1\n",
    "\n",
    "t_text, p_rnn, p_lstm, p_gru = predict_sequence(X_set, y_set, nb_samples, n)    \n",
    "\n",
    "print('\\n Accuracy with test data')\n",
    "# print(\"Correct\", t_text)\n",
    "print(\"rnn: accuracy = \", p_accuracy(t_text,p_rnn))\n",
    "print(\"lstm: accuracy = \", p_accuracy(t_text,p_lstm))\n",
    "print(\"gru: accuracy = \", p_accuracy(t_text,p_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU scores calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def calc_bleu(ref, hyp):\n",
    "    BLEU1 = corpus_bleu(ref, hyp, weights=(1.0, 0, 0, 0))\n",
    "    BLEU2 = corpus_bleu(ref, hyp, weights=(0.5, 0.5, 0, 0))\n",
    "    BLEU3 = corpus_bleu(ref, hyp, weights=(0.3, 0.3, 0.3, 0))\n",
    "    return(BLEU1,BLEU2,BLEU3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "X_set = X_train\n",
    "y_set = y_train\n",
    "\n",
    "nb_samples = 1000\n",
    "i_samples = [np.random.randint(2, len(X_set)-1) for p in range(0, nb_samples)]\n",
    "n = 10\n",
    "\n",
    "t_text, p_rnn, p_lstm, p_gru = predict_sequence(X_set, y_set, nb_samples, n)    \n",
    "\n",
    "print('\\n BLEU score with training data')\n",
    "# print(\"Correct\\n\", t_text)\n",
    "# print(\"rnn\\n \",p_rnn)\n",
    "# print(\"lstm\\n\", p_lstm)\n",
    "# print(\"gru\\n\", p_gru)\n",
    "\n",
    "ref = np.array(t_text).reshape(nb_samples,1,-1)\n",
    "print('lstm')\n",
    "print(calc_bleu(ref, p_lstm))\n",
    "print('gru')\n",
    "print(calc_bleu(ref, p_gru))\n",
    "print('rnn')\n",
    "print(calc_bleu(ref, p_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "X_set = X_test\n",
    "y_set = y_test\n",
    "\n",
    "nb_samples = len(y_test)\n",
    "i_samples = [np.random.randint(2, len(X_set)-1) for p in range(0, nb_samples)]\n",
    "n = 10\n",
    "\n",
    "t_text, p_rnn, p_lstm, p_gru = predict_sequence(X_set, y_set, nb_samples, n)    \n",
    "\n",
    "print('\\n BLEU score with test data')\n",
    "print(\"Correct\\n\", t_text)\n",
    "# print(\"rnn\\n \",p_rnn)\n",
    "print(\"lstm\\n\", p_lstm)\n",
    "# print(\"gru\\n\", p_gru)\n",
    "\n",
    "ref = np.array(t_text).reshape(nb_samples,1,-1)\n",
    "print('lstm')\n",
    "print(calc_bleu(ref, p_lstm))\n",
    "print('gru')\n",
    "print(calc_bleu(ref, p_gru))\n",
    "print('rnn')\n",
    "print(calc_bleu(ref, p_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(p_lstm)):\n",
    "    print(ref[i][0])\n",
    "    print(p_lstm[i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Quran_LSTM_Sample.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
